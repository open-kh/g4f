{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you'll need to install the necessary dependencies, such as PyTorch and the Transformer library. You can do this using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you'll need to download a pre-trained Transformer model. You can do this using the transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline, BertTokenizer, BertModel, Trainer, BertConfig, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"moreh/MoMo-70B-lora-1.8.6-DPO\",\"bert-large-uncased-whole-word-masking\",\"bert-base-uncased\"]\n",
    "model_name = model_names[1]\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll need to prepare your dataset for fine-tuning. This will involve tokenizing your text data and creating a CSV file that contains the input text and the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask', model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1581384241580963,\n",
       "  'token': 4827,\n",
       "  'token_str': 'fashion',\n",
       "  'sequence': \"hello i'm a fashion model.\"},\n",
       " {'score': 0.10551061481237411,\n",
       "  'token': 3104,\n",
       "  'token_str': 'cover',\n",
       "  'sequence': \"hello i'm a cover model.\"},\n",
       " {'score': 0.08340441435575485,\n",
       "  'token': 3287,\n",
       "  'token_str': 'male',\n",
       "  'sequence': \"hello i'm a male model.\"},\n",
       " {'score': 0.036382000893354416,\n",
       "  'token': 3565,\n",
       "  'token_str': 'super',\n",
       "  'sequence': \"hello i'm a super model.\"},\n",
       " {'score': 0.036095913499593735,\n",
       "  'token': 2327,\n",
       "  'token_str': 'top',\n",
       "  'sequence': \"hello i'm a top model.\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"Hello I'm a [MASK] model.\")\n",
    "# unmasker(\"The man worked as a [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# model = BertModel.from_pretrained(model_name)\n",
    "from transformers import BertForMaskedLM, BertTokenizer, BertConfig\n",
    "\n",
    "config = BertConfig.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name, config=config)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/train-00000-of-00001-45c83a9a6160af42.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Your goal is to determine the relationship bet...</td>\n",
       "      <td>Sentence 1: For his hypotension, autonomic tes...</td>\n",
       "      <td>Entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the provided text, your objective is to rec...</td>\n",
       "      <td>The first product of ascorbate oxidation , the...</td>\n",
       "      <td>The : O\\nfirst : O\\nproduct : O\\nof : O\\nascor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you have expertise in healthcare, assist us...</td>\n",
       "      <td>i am having acute gas problem which i am feeli...</td>\n",
       "      <td>Dear-thanks for using our service and will try...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Being a doctor, your task is to answer the med...</td>\n",
       "      <td>###Question: What is the relation between Hous...</td>\n",
       "      <td>###Answer: Does not bath self (finding) interp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In your role as a medical professional, addres...</td>\n",
       "      <td>Hi I have the last few weeks had a sharp pain ...</td>\n",
       "      <td>Thank you. Upper abdominal pain with anemia in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205043</th>\n",
       "      <td>Your role involves answering medical questions...</td>\n",
       "      <td>Ok, so I have a friend that is 30 years old an...</td>\n",
       "      <td>Hi, Why the detection may have been late. I mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205044</th>\n",
       "      <td>Your task is to determine the relationships be...</td>\n",
       "      <td>Meds at home : @treatment$ , Levoxyl , @treatm...</td>\n",
       "      <td>No Relations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205045</th>\n",
       "      <td>Given your profession as a doctor, please prov...</td>\n",
       "      <td>###Question: What is the relation between Open...</td>\n",
       "      <td>###Answer: Open fine needle biopsy of liver (p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205046</th>\n",
       "      <td>Your goal as an annotator is to recognize clin...</td>\n",
       "      <td>Received 2 mg dilaudid for pain o/n .</td>\n",
       "      <td>Received : O\\n2 : O\\nmg : O\\ndilaudid : B-TREA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205047</th>\n",
       "      <td>Your role involves answering medical questions...</td>\n",
       "      <td>My femur fracture happened Jan. 2014. I had dy...</td>\n",
       "      <td>Hello healthcare user, dynamisation is done in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205048 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              instruction   \n",
       "0       Your goal is to determine the relationship bet...  \\\n",
       "1       In the provided text, your objective is to rec...   \n",
       "2       If you have expertise in healthcare, assist us...   \n",
       "3       Being a doctor, your task is to answer the med...   \n",
       "4       In your role as a medical professional, addres...   \n",
       "...                                                   ...   \n",
       "205043  Your role involves answering medical questions...   \n",
       "205044  Your task is to determine the relationships be...   \n",
       "205045  Given your profession as a doctor, please prov...   \n",
       "205046  Your goal as an annotator is to recognize clin...   \n",
       "205047  Your role involves answering medical questions...   \n",
       "\n",
       "                                                    input   \n",
       "0       Sentence 1: For his hypotension, autonomic tes...  \\\n",
       "1       The first product of ascorbate oxidation , the...   \n",
       "2       i am having acute gas problem which i am feeli...   \n",
       "3       ###Question: What is the relation between Hous...   \n",
       "4       Hi I have the last few weeks had a sharp pain ...   \n",
       "...                                                   ...   \n",
       "205043  Ok, so I have a friend that is 30 years old an...   \n",
       "205044  Meds at home : @treatment$ , Levoxyl , @treatm...   \n",
       "205045  ###Question: What is the relation between Open...   \n",
       "205046              Received 2 mg dilaudid for pain o/n .   \n",
       "205047  My femur fracture happened Jan. 2014. I had dy...   \n",
       "\n",
       "                                                   output  \n",
       "0                                              Entailment  \n",
       "1       The : O\\nfirst : O\\nproduct : O\\nof : O\\nascor...  \n",
       "2       Dear-thanks for using our service and will try...  \n",
       "3       ###Answer: Does not bath self (finding) interp...  \n",
       "4       Thank you. Upper abdominal pain with anemia in...  \n",
       "...                                                   ...  \n",
       "205043  Hi, Why the detection may have been late. I mu...  \n",
       "205044                                       No Relations  \n",
       "205045  ###Answer: Open fine needle biopsy of liver (p...  \n",
       "205046  Received : O\\n2 : O\\nmg : O\\ndilaudid : B-TREA...  \n",
       "205047  Hello healthcare user, dynamisation is done in...  \n",
       "\n",
       "[205048 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df.iloc[: 1000]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Replace me by any text you'd like.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put model in train mode and move to GPU if available\n",
    "model.train()\n",
    "device = torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "from queue import Queue\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "df = pd.read_parquet(\"data/train-00000-of-00001-45c83a9a6160af42.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "train_data = [\n",
    "    ['Summarize this', 'The quick brown fox jumped over the lazy dog', 'The text describes a fox quickly jumping over a lazy dog.'],\n",
    "    ['Translate to French', 'The house is blue', 'La maison est bleue']  \n",
    "]\n",
    "# Load CSV data\n",
    "train_data = df.values[:100]\n",
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.queue = Queue()\n",
    "        for item in data:\n",
    "            self.queue.put(item)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.queue.get()\n",
    "\n",
    "# Custom collate function to handle variable batch sizes\n",
    "def custom_collate(batch):\n",
    "    batch_size = len(batch)\n",
    "    max_len = max(len(item[1]) for item in batch)\n",
    "    \n",
    "    input_ids = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
    "    labels = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
    "\n",
    "    for i, item in enumerate(batch):\n",
    "        instruction, input_text, output_text = item\n",
    "        input_encoding = tokenizer.encode(instruction + tokenizer.mask_token + input_text, return_tensors='pt')\n",
    "        label_encoding = tokenizer.encode(output_text + tokenizer.eos_token, return_tensors='pt', padding=True)\n",
    "\n",
    "        input_ids[i, :input_encoding.size(1)] = input_encoding[0]\n",
    "        labels[i, :label_encoding.size(1)] = label_encoding[0]\n",
    "\n",
    "    return {'input_ids': input_ids, 'labels': labels}\n",
    "\n",
    "# Create custom dataset and dataloader\n",
    "custom_dataset = CustomDataset(train_data)\n",
    "custom_dataloader = DataLoader(custom_dataset, batch_size=2, collate_fn=custom_collate)\n",
    "\n",
    "# Fine-tune \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_encodings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set model to training mode\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_encodings\u001b[49m:  \n\u001b[1;32m      5\u001b[0m        \u001b[38;5;66;03m# Adjust batch size\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][:, :\u001b[38;5;241m3\u001b[39m]  \u001b[38;5;66;03m# Assuming 3 is the desired batch size\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         labels \u001b[38;5;241m=\u001b[39m item\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_encodings' is not defined"
     ]
    }
   ],
   "source": [
    "model.train()  # Set model to training mode\n",
    "\n",
    "for epoch in range(3):\n",
    "    for item in train_encodings:  \n",
    "       # Adjust batch size\n",
    "        input_ids = item['input_ids'][:, :3]  # Assuming 3 is the desired batch size\n",
    "        labels = item.get('labels', None)\n",
    "        if labels is not None:\n",
    "            labels = labels[:, :3]  # Assuming 3 is the desired batch size\n",
    "\n",
    "        outputs = model(input_ids, labels=labels)  \n",
    "        loss = outputs.loss\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "model.eval()  # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"NoneType\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set model to training mode\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m custom_dataloader:  \n\u001b[1;32m      5\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m         labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 26\u001b[0m, in \u001b[0;36mcustom_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     24\u001b[0m instruction, input_text, output_text \u001b[38;5;241m=\u001b[39m item\n\u001b[1;32m     25\u001b[0m input_encoding \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(instruction \u001b[38;5;241m+\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mmask_token \u001b[38;5;241m+\u001b[39m input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m label_encoding \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[43moutput_text\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token\u001b[49m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m input_ids[i, :input_encoding\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m=\u001b[39m input_encoding[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     29\u001b[0m labels[i, :label_encoding\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m=\u001b[39m label_encoding[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"NoneType\") to str"
     ]
    }
   ],
   "source": [
    "model.train()  # Set model to training mode\n",
    "\n",
    "for epoch in range(3):\n",
    "    for batch in custom_dataloader:  \n",
    "        input_ids = batch['input_ids']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        outputs = model(input_ids, labels=labels)  \n",
    "        loss = outputs.loss\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "model.eval()  # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text generation  \n",
    "instruction = 'Summarize this'\n",
    "text = 'The quick brown fox jumped over the lazy dog'\n",
    "input_ids = tokenizer.encode(instruction + tokenizer.mask_token + text,  \n",
    "                             return_tensors='pt') \n",
    "outputs = model(input_ids)\n",
    "predictions = torch.argmax(outputs.logits, dim=2)\n",
    "generated_text = tokenizer.decode(predictions[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode data\n",
    "def encode(instruction, input, output=None):\n",
    "    input_ids = tokenizer.encode(instruction + tokenizer.sep_token + input, \n",
    "                                 return_tensors='pt')\n",
    "    if output:\n",
    "        output_ids = tokenizer.encode(output, return_tensors='pt') \n",
    "    return {'input_ids': input_ids, 'labels': output_ids} if output else {'input_ids': input_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (677 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and encode data\n",
    "def encode(instruction, input, output=None):\n",
    "    input_ids = tokenizer.encode(instruction + tokenizer.mask_token + input, \n",
    "                                 return_tensors='pt')\n",
    "    if output:\n",
    "        output_ids = tokenizer.encode(output, return_tensors='pt', padding=True) \n",
    "        return {'input_ids': input_ids, 'labels': output_ids}\n",
    "    return {'input_ids': input_ids}\n",
    "\n",
    "train_encodings = [encode(ins, inp, out) for ins, inp, out in train_data]\n",
    "\n",
    "# Fine-tune \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()  # Set model to training mode\n",
    "\n",
    "for epoch in range(3):\n",
    "    for item in train_encodings:  \n",
    "       # Adjust batch size\n",
    "        input_ids = item['input_ids'][:, :3]  # Assuming 3 is the desired batch size\n",
    "        labels = item.get('labels', None)\n",
    "        if labels is not None:\n",
    "            labels = labels[:, :3]  # Assuming 3 is the desired batch size\n",
    "\n",
    "        outputs = model(input_ids, labels=labels)  \n",
    "        loss = outputs.loss\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "model.eval()  # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] summarize sds [SEP] the quick brown fox jumped over the lazy dogs [SEP] # # #\n"
     ]
    }
   ],
   "source": [
    "# Sample text generation  \n",
    "instruction = 'Summarize sds'\n",
    "text = 'The quick brown fox jumped over the lazy dogs'\n",
    "input_ids = tokenizer.encode(instruction + tokenizer.sep_token + text,  \n",
    "                             return_tensors='pt')\n",
    "# outputs = model(input_ids)\n",
    "# predictions = torch.argmax(outputs.logits, dim=2)\n",
    "# generated_text = tokenizer.decode(predictions[0], skip_special_tokens=True)\n",
    "# print(generated_text)\n",
    "\n",
    "generated = model.generate(input_ids)\n",
    "# generated\n",
    "print(tokenizer.decode(generated[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "tokenized_data = tokenizer([inputs], padding=\"max_length\", truncation=True, max_length=32)\n",
    "\n",
    "input_ids = tokenized_data.input_ids.squeeze().tolist()  \n",
    "attention_mask = tokenized_data.attention_mask.squeeze().tolist()\n",
    "\n",
    "# Only take first 1000 samples\n",
    "input_ids = input_ids[:1000]\n",
    "attention_mask = attention_mask[:1000]\n",
    "\n",
    "fine_tune_data = pd.DataFrame({\n",
    "    \"input_ids\": input_ids,\n",
    "    \"attention_mask\": attention_mask,\n",
    "    \"labels\": outputs\n",
    "})\n",
    "\n",
    "fine_tune_data.to_csv(\"fine_tune_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenizer.batch_encode_plus(inputs,\n",
    "                                            padding=\"max_length\",\n",
    "                                            truncation=True,\n",
    "                                            max_length=32,\n",
    "                                            return_attention_mask=True,\n",
    "                                            return_tensors=\"pt\")\n",
    "\n",
    "input_ids = tokenized_data[\"input_ids\"].detach().cpu().numpy().tolist()\n",
    "attention_mask = tokenized_data[\"attention_mask\"].detach().cpu().numpy().tolist()\n",
    "# print(len(input_ids), len(attention_mask))\n",
    "\n",
    "# Flatten 2D array to 1D\n",
    "input_ids = [item for sublist in input_ids for item in sublist]  \n",
    "attention_mask = [item for sublist in attention_mask for item in sublist]\n",
    "\n",
    "print(len(input_ids), len(attention_mask), len(outputs))\n",
    "\n",
    "# fine_tune_data = pd.DataFrame({\n",
    "#     \"input_ids\": input_ids,\n",
    "#     \"attention_mask\": attention_mask, \n",
    "#     \"labels\": outputs\n",
    "# })\n",
    "\n",
    "# fine_tune_data.to_csv(\"fine_tune_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    save_total_limit=2,\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    save_strategy=\"steps\",\n",
    "    save_on_each_node=True,\n",
    ")\n",
    "\n",
    "# Set up the Trainer\n",
    "trainer = Trainer(\n",
    "    model=AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=8),\n",
    "    args=training_args,\n",
    "    train_dataset=fine_tune_data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=lambda pred: {\"accuracy\": accuracy_score(pred.label_ids, pred.predictions.argmax(-1))},\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(input_ids):\n",
    "        # Move batch to GPU\n",
    "        batch = {k: v.type(torch.long).to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss =  criterion(outputs.logits.view(-1, config.vocab_size), \n",
    "                         batch['labels'].view(-1))\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        if step % 100 == 0:\n",
    "            print(f'Epoch: {epoch} | Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fine-tuned model\n",
    "output_dir = '/path/to/save/' \n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(df['instruction'].values.tolist(), return_tensors='pt')\n",
    "output_ids = tokenizer.encode(df['output'].values.tolist(), return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the input and output sequences\n",
    "max_length = 32\n",
    "padded_input_ids = torch.zeros((len(df), max_length)).to(device)\n",
    "padded_output_ids = torch.zeros((len(df), max_length)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training loop\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BertModel.from_pretrained(model_name, num_labels=10000).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (input_ids, output_ids) in enumerate(zip(padded_input_ids, padded_output_ids)):\n",
    "        input_ids = input_ids.to(device)\n",
    "        output_ids = output_ids.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = loss_fn(outputs, output_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(df)}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch_idx, (input_ids, output_ids) in enumerate(zip(padded_input_ids, padded_output_ids)):\n",
    "        input_ids = input_ids.to(device)\n",
    "        output_ids = output_ids.to(device)\n",
    "        outputs = model(input_ids)\n",
    "        loss = loss_fn(outputs, output_ids)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        correct += (predicted == output_ids).sum().item()\n",
    "\n",
    "accuracy = correct / len(df)\n",
    "print(f'Test Loss: {test_loss / len(df)}')\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_data = tokenizer.batch_encode_plus(df[\"text\"],\n",
    "                                            padding=\"max_length\",\n",
    "                                            truncation=True,\n",
    "                                            max_length=32,\n",
    "                                            return_attention_mask=True,\n",
    "                                            return_tensors=\"pt\")\n",
    "\n",
    "# Create a CSV file for fine-tuning\n",
    "fine_tune_data = pd.DataFrame({\"input_ids\": tokenized_data[\"input_ids\"],\n",
    "                             \"attention_mask\": tokenized_data[\"attention_mask\"],\n",
    "                             \"labels\": df[\"label\"]})\n",
    "fine_tune_data.to_csv(\"fine_tune_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can fine-tune the Transformer model using the Trainer class from the transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    save_total_limit=2,\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    save_strategy=\"steps\",\n",
    "    save_on_each_node=True,\n",
    ")\n",
    "\n",
    "# Set up the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=fine_tune_data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=lambda pred: {\"accuracy\": accuracy_score(pred.label_ids, pred.predictions.argmax(-1))},\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the fine-tuning process is complete, you can use the trained model to make predictions on new, unseen text data. Here's an example of how you could do this using the transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=8)\n",
    "\n",
    "# Tokenize the input text\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "input_text = \"This is a sample input text.\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Make predictions on the input text\n",
    "predictions = model(input_ids)\n",
    "\n",
    "# Get the predicted label\n",
    "predicted_label = predictions.argmax(-1)\n",
    "\n",
    "# Print the predicted label\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we first load the trained model and tokenizer. We then tokenize the input text and pass it through the model to get the predicted label.\n",
    "\n",
    "You can also use the predict method of the Trainer class to make predictions on new data. Here's an example of how you could do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=8)\n",
    "\n",
    "# Tokenize the input text\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "input_text = \"This is a sample input text.\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Create a dataset for the input text\n",
    "input_dataset = pd.DataFrame({\"input_ids\": input_ids, \"attention_mask\": tokenizer.encode(input_text, return_tensors=\"pt\", max_length=32, padding=\"max_length\", truncation=True)})\n",
    "\n",
    "# Set up the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=fine_tune_data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=lambda pred: {\"accuracy\": accuracy_score(pred.label_ids, pred.predictions.argmax(-1))},\n",
    ")\n",
    "\n",
    "# Make predictions on the input text\n",
    "predictions = trainer.predict(input_dataset)\n",
    "\n",
    "# Get the predicted label\n",
    "predicted_label = predictions.argmax(-1)\n",
    "\n",
    "# Print the predicted label\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we first load the trained model and tokenizer. We then tokenize the input text and create a dataset for the input text. We set up the Trainer and use the predict method to make predictions on the input text. Finally, we get the predicted label and print it to the console.\n",
    "\n",
    "Keep in mind that the specifics of how to use the trained model will depend on your specific use case and the format of your data. You may need to modify the code to fit your specific needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can use a BERT-based model for language translation tasks. BERT has been pre-trained on a large corpus of text data and has learned to encode language in a way that is useful for a wide range of NLP tasks, including language translation.\n",
    "\n",
    "To use a BERT-based model for language translation, you would need to fine-tune the model on a large corpus of text data in the source language and the target language. This would involve adding a new output layer on top of the pre-trained BERT model and training the whole network to predict the correct translation in the target language.\n",
    "\n",
    "Here's an example of how you could fine-tune a BERT-based model for language translation using the transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=8)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define the training data\n",
    "train_data = [\n",
    "    {\"input_ids\": tokenizer.encode(\"This is a sample input sentence in English\", return_tensors=\"pt\"), \"attention_mask\": tokenizer.encode(\"This is a sample input sentence in English\", return_tensors=\"pt\", max_length=32, padding=\"max_length\", truncation=True)},\n",
    "    {\"input_ids\": tokenizer.encode(\"This is a sample input sentence in Spanish\", return_tensors=\"pt\"), \"attention_mask\": tokenizer.encode(\"This is a sample input sentence in Spanish\", return_tensors=\"pt\", max_length=32, padding=\"max_length\", truncation=True)}\n",
    "]\n",
    "\n",
    "# Define the labels\n",
    "labels = [1, 1]\n",
    "\n",
    "# Create a dataset from the training data\n",
    "train_dataset = pd.DataFrame(train_data, columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Set up the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=lambda pred: {\"accuracy\": accuracy_score(pred.label_ids, pred.predictions.argmax(-1))},\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the trained model to translate text\n",
    "def translate(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    input_dataset = pd.DataFrame({\"input_ids\": input_ids, \"attention_mask\": tokenizer.encode(input_text, return_tensors=\"pt\", max_length=32, padding=\"max_length\", truncation=True)})\n",
    "    predictions = trainer.predict(input_dataset)\n",
    "    predicted_label = predictions.argmax(-1)\n",
    "    return predicted_label\n",
    "\n",
    "# Test the translation model\n",
    "input_text = \"I love to travel to new places.\"\n",
    "translated_text = translate(input_text)\n",
    "print(f\"Translated text: {translated_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
