{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-oww1h75o\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-oww1h75o\n",
      "  Resolved https://github.com/huggingface/transformers to commit 66c240f3c950612fa05b2e14c85d4b86c88e473e\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.99-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.14.3-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.1/519.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from transformers==4.32.0.dev0) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in ./venv/lib/python3.11/site-packages (from transformers==4.32.0.dev0) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.11/site-packages (from transformers==4.32.0.dev0) (1.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.11/site-packages (from transformers==4.32.0.dev0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from transformers==4.32.0.dev0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.11/site-packages (from transformers==4.32.0.dev0) (2023.6.3)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.11/site-packages (from transformers==4.32.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./venv/lib/python3.11/site-packages (from transformers==4.32.0.dev0) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./venv/lib/python3.11/site-packages (from transformers==4.32.0.dev0) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.11/site-packages (from transformers==4.32.0.dev0) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./venv/lib/python3.11/site-packages (from datasets) (12.0.1)\n",
      "Collecting dill<0.3.8,>=0.3.0\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in ./venv/lib/python3.11/site-packages (from datasets) (1.5.3)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in ./venv/lib/python3.11/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.11/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.32.0.dev0) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests->transformers==4.32.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests->transformers==4.32.0.dev0) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests->transformers==4.32.0.dev0) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./venv/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.32.0.dev0-py3-none-any.whl size=7450897 sha256=ee02db8fa759d4e69d52b9ea39d91348e3b4c00af5bf3887125689a386f1db84\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-uzw_ijuu/wheels/04/a3/f1/b88775f8e1665827525b19ac7590250f1038d947067beba9fb\n",
      "Successfully built transformers\n",
      "Installing collected packages: sentencepiece, xxhash, dill, multiprocess, transformers, datasets\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.31.0\n",
      "    Uninstalling transformers-4.31.0:\n",
      "      Successfully uninstalled transformers-4.31.0\n",
      "Successfully installed datasets-2.14.3 dill-0.3.7 multiprocess-0.70.15 sentencepiece-0.1.99 transformers-4.32.0.dev0 xxhash-3.3.0\n"
     ]
    }
   ],
   "source": [
    "# Following pip packages need to be installed:\n",
    "!pip install git+https://github.com/huggingface/transformers sentencepiece datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soundfile\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.0 in ./venv/lib/python3.11/site-packages (from soundfile) (1.15.1)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.11/site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Installing collected packages: soundfile\n",
      "Successfully installed soundfile-0.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load feature extractor for 'microsoft/speecht5_tts'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'microsoft/speecht5_tts' is the correct path to a directory containing a preprocessor_config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/projects/baichat/gpt4free/venv/lib/python3.11/site-packages/transformers/feature_extraction_utils.py:488\u001b[0m, in \u001b[0;36mFeatureExtractionMixin.get_feature_extractor_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    487\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 488\u001b[0m     resolved_feature_extractor_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    489\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    490\u001b[0m         feature_extractor_file,\n\u001b[1;32m    491\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    492\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    493\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    494\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    495\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    496\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    497\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    498\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    499\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    502\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cached_file() got an unexpected keyword argument 'token'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m processor \u001b[39m=\u001b[39m SpeechT5Processor\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mmicrosoft/speecht5_tts\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m SpeechT5ForTextToSpeech\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmicrosoft/speecht5_tts\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m vocoder \u001b[39m=\u001b[39m SpeechT5HifiGan\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmicrosoft/speecht5_hifigan\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/baichat/gpt4free/venv/lib/python3.11/site-packages/transformers/processing_utils.py:215\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m use_auth_token \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39muse_auth_token\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    213\u001b[0m \u001b[39mif\u001b[39;00m use_auth_token \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m--> 215\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFutureWarning\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     )\n\u001b[1;32m    217\u001b[0m     \u001b[39mif\u001b[39;00m token \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    219\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`token` and `use_auth_token` are both specified. Please set only the argument `token`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m         )\n",
      "File \u001b[0;32m~/projects/baichat/gpt4free/venv/lib/python3.11/site-packages/transformers/processing_utils.py:259\u001b[0m, in \u001b[0;36m_get_arguments_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m args \u001b[39m=\u001b[39m []\n\u001b[1;32m    258\u001b[0m \u001b[39mfor\u001b[39;00m attribute_name \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mattributes:\n\u001b[0;32m--> 259\u001b[0m     class_name \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mattribute_name\u001b[39m}\u001b[39;00m\u001b[39m_class\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    260\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(class_name, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    261\u001b[0m         classes \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mgetattr\u001b[39m(transformers_module, n) \u001b[39mif\u001b[39;00m n \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m class_name)\n",
      "File \u001b[0;32m~/projects/baichat/gpt4free/venv/lib/python3.11/site-packages/transformers/feature_extraction_utils.py:365\u001b[0m, in \u001b[0;36mFeatureExtractionMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mif\u001b[39;00m token \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    363\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m token\n\u001b[0;32m--> 365\u001b[0m feature_extractor_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_feature_extractor_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    367\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_dict(feature_extractor_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/projects/baichat/gpt4free/venv/lib/python3.11/site-packages/transformers/feature_extraction_utils.py:506\u001b[0m, in \u001b[0;36mFeatureExtractionMixin.get_feature_extractor_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    504\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    505\u001b[0m         \u001b[39m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    507\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load feature extractor for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m it from \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    509\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m same name. Otherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m directory containing a \u001b[39m\u001b[39m{\u001b[39;00mFEATURE_EXTRACTOR_NAME\u001b[39m}\u001b[39;00m\u001b[39m file\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m         )\n\u001b[1;32m    513\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    514\u001b[0m     \u001b[39m# Load feature_extractor dict\u001b[39;00m\n\u001b[1;32m    515\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(resolved_feature_extractor_file, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m reader:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load feature extractor for 'microsoft/speecht5_tts'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'microsoft/speecht5_tts' is the correct path to a directory containing a preprocessor_config.json file"
     ]
    }
   ],
   "source": [
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "\n",
    "inputs = processor(text=\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "# load xvector containing speaker's voice characteristics from a dataset\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "\n",
    "speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.write(\"speech.wav\", speech.numpy(), samplerate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "# API_KEY=\"sk-WTeLd6cpQJzWBwqrwDDWkZGwiyg4IQ0dUpReZc8v59Sd3N9l\"\n",
    "# MODEL_ID=\"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "# MODEL_ID=\"stabilityai/stable-diffusion-xl-base-0.9\"\n",
    "MODEL_ID=\"CompVis/stable-diffusion-v1-4\"\n",
    "API_KEY=\"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\"\n",
    "headers = {\n",
    "  \"Accept\": \"application/json\",\n",
    "  \"Content-Type\": \"application/json\",\n",
    "  \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "}\n",
    "API_URL = f\"https://api-inference.huggingface.co/models/{MODEL_ID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_GEN = \"Rope ladder from space station, astronauts climbing the ladder, UHD, 32K, Hyper, Dynamic Colors, Cinematic scene\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BODY = {\n",
    "  \"width\": 512,\n",
    "  \"height\": 512,\n",
    "  \"steps\": 50,\n",
    "  \"seed\": 0,\n",
    "  \"cfg_scale\": 7,\n",
    "  \"samples\": 1,\n",
    "  \"style_preset\": \"enhance\",\n",
    "  \"text_prompts\": [\n",
    "    {\n",
    "      \"text\": f\"{TEXT_GEN}\",\n",
    "      \"weight\": 1\n",
    "    },\n",
    "    {\n",
    "      \"text\": \"reduce noise\",\n",
    "      \"weight\": -1\n",
    "    }\n",
    "  ],\n",
    "  \"inputs\": TEXT_GEN\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_GEN = \"A high tech solarpunk utopia in the Amazon rainforest\"\n",
    "BODY = {\n",
    "  \"width\": 512,\n",
    "  \"height\": 512,\n",
    "  \"steps\": 50,\n",
    "  \"seed\": 0,\n",
    "  \"cfg_scale\": 7,\n",
    "  \"samples\": 1,\n",
    "  \"style_preset\": \"enhance\",\n",
    "  \"text_prompts\": [\n",
    "    {\n",
    "      \"text\": f\"{TEXT_GEN}\",\n",
    "      \"weight\": 1\n",
    "    },\n",
    "    {\n",
    "      \"text\": \"reduce noise\",\n",
    "      \"weight\": -1\n",
    "    }\n",
    "  ],\n",
    "  \"inputs\": TEXT_GEN\n",
    "}\n",
    "image_bytes = query(BODY)\n",
    "image = Image.open(io.BytesIO(image_bytes))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, image in enumerate(data[\"artifacts\"]):\n",
    "    img64 = image[\"base64\"]\n",
    "    imgID = image[\"seed\"]\n",
    "    with open(f\"./out/txt2img_{imgID}.png\", \"wb\") as f:\n",
    "        f.write(base64.b64decode(img64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.content\n",
    "image_bytes = query({\n",
    "\t\"inputs\": \"Astronaut riding a horse\",\n",
    "})\n",
    "# You can access the image with PIL.Image for example\n",
    "# import io\n",
    "# from PIL import Image\n",
    "# image = Image.open(io.BytesIO(image_bytes))\n",
    "image_bytes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
